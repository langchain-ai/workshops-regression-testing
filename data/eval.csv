question,answer
"How do I set up a retrieval-augmented generation chain using LCEL?","Here is an example RAG chain using LCEL: \n\nfrom operator import itemgetter\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nmodel = ChatOpenAI()\n\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nchain.invoke(\"where did harrison work?\")"
"How can I use a prompt and model to create a chain in LCEL?","Here is an example of a prompt + LLM chain using LCEL: \nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")\nmodel = ChatOpenAI()\nchain = prompt | model\n\nchain.invoke({\"foo\": \"bears\"})"
"How can I add memory to an arbitrary chain using LCEL?","Here is an example adding memory to a chain: \nfrom operator import itemgetter\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\n\nmodel = ChatOpenAI()\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful chatbot\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\nmemory = ConversationBufferMemory(return_messages=True)\n\nchain = (\n    RunnablePassthrough.assign(\n        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n    )\n    | prompt\n    | model\n)\n\ninputs = {\"input\": \"hi im bob\"}\nresponse = chain.invoke(inputs)"
"I've defined a runnable chain = prompt | model. How can I look at the input schema?","All runnables expose input and output schemas to inspect the inputs and outputs. input_schema is an input Pydantic model auto-generated from the structure of the Runnable. You can call .schema() on it to obtain a JSONSchema representation of any runnable: # The input schema of the chain is the input schema of its first part, the prompt. chain.input_schema.schema()"
"I have a runnable, chain, and am passing in a map w/ {'question' 'where did harrison work', 'language': 'italian'}. How can I extract the value of 'language' to pass to my prompt?","For this example, we can use itemgetter to extract specific values from the map. Here is an example: from operator import itemgetter from langchain.chat_models import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.prompts import ChatPromptTemplate from langchain.vectorstores import FAISS from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough vectorstore = FAISS.from_texts(['harrison worked at kensho'], embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever() template = 'Answer the question based only on the following context: {context} Question: {question} Answer in the following language: {language}' prompt = ChatPromptTemplate.from_template(template) chain = ( { 'context': itemgetter('question') | retriever, 'question': itemgetter('question'), 'language': itemgetter('language'), } | prompt | model | StrOutputParser() ) chain.invoke({'question': 'where did harrison work', 'language': 'italian'})"
"I am passing text key 'foo' to my prompt and want to process it with a function, process_text(...), prior to the prompt. How can I do this using LCEL?","You can use a RunnableLambda to apply a function to the value of foo: chain = ( { 'a': itemgetter('foo') | RunnableLambda(process_text), | RunnableLambda(multiple_length_function), } | prompt | model )"
"I have two chains, conversation_chain and retriever. I want to use conversation_chain if the input has 'chat_history' and I want to use retriever if the input has 'question'. How can I do this in LCEL using RunnableBranch","Using RunnableBranch: RunnableBranch( ( RunnableLambda(lambda x: bool(x.get('chat_history')) ), conversation_chain ), ( RunnableLambda(itemgetter('question')) | retriever ) )"
"My map contains the key 'question'. What is the difference between using itemgetter('question'), lambda x: x['question'], and x.get('question')?","Itemgetter can be used as shorthand to extract specific keys from the map. In the context of a map operation, the lambda function is applied to each element in the input map and the function returns the value associated with the key 'question'. (get) is safer for accessing values in a dictionary because it handles the case where the key might not exist."
"I'm invoking a chain with a map that contain {'question': 'how do I use Anthropic?'}. The full chain definition is full_chain = {'question': lambda x: x['question']} | sub_chain. Why is a lambda used?","The lambda function is an anonymous function that takes one argument, x, and returns x['question']. In the context of a map operation, this function is applied to each element in the input iterable. If the input is a dictionary (map), as in this case, x would be this map, and the function returns the value associated with the key 'question'."
"How can I create the self-query retriever query-construction chain in LCEL?","To create a self-query retriever query-construction chain in LCEL: from langchain.chains.query_constructor.base import ( StructuredQueryOutputParser, get_query_constructor_prompt, ) prompt = get_query_constructor_prompt(document_content_description, metadata_field_info,) output_parser = StructuredQueryOutputParser.from_components() query_constructor = prompt | llm | output_parser query_constructor.invoke({ 'query': 'What are some sci-fi movies from the 90's directed by Luc Besson about taxi drivers' }) StructuredQuery(query='taxi driver', filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='science fiction'), Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Luc Besson')]), limit=None)"